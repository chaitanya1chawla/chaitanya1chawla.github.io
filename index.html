<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="en">

<head>
	<!-- <meta name=viewport content="width=800"> -->
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
	<meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
	<script type="text/javascript" src="js/hidebib.js"></script>
	<style type="text/css">
		/* Color scheme stolen from Sergey Karayev */
		
		hr {
		border: 0;
		height: 0;
		border-top: 1px solid rgba(0, 0, 0, 0.1);
		border-bottom: 1px solid rgba(255, 255, 255, 0.3);
		}

		img {
		border-radius: 5%;
		}

		a {
		color: #1772d0;
		text-decoration: none;
		}
		
		a:focus,
		a:hover {
		color: #f09228;
		text-decoration: none;
		}
		
		body,
		td,
		th,
		tr,
		p,
		/* a {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 14px
		}
		
		strong {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 14px;
		}
		
		heading {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 30px;
		}
		
		papertitle {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 14px;
		font-weight: 700;
		margin-bottom: 10cm;
		}
		
		name {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 32px;
		} */
		a {
		font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
		font-size: 16px;
		font-weight: 400
		}

		strong {
		font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
		font-size: 16px;
		font-weight: 600
		}

		heading {
		font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
		font-size: 17px;
		font-weight: 600
		}

		papertitle {
		font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
		font-size: 16px;
		font-weight: 600
		}

		name {
		font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
		font-size: 32px;
		font-weight: 400
		}
		.zero {
		width: 160px;
		height: 80px;
		position: relative;
		}

		.one {
		width: 160px;
		height: 160px;
		position: relative;
		}
		
		.two {
		width: 160px;
		height: 160px;
		position: absolute;
		transition: opacity .2s ease-in-out;
		-moz-transition: opacity .2s ease-in-out;
		-webkit-transition: opacity .2s ease-in-out;
		}
		
		.fade {
		transition: opacity .2s ease-in-out;
		-moz-transition: opacity .2s ease-in-out;
		-webkit-transition: opacity .2s ease-in-out;
		}
		
		mid {
		font-size: 40px;
		position:relative;
		top:2px;
		}

		span.highlight {
		background-color: #ffffd0;
		}


	#summary:hover + #detail, #detail:hover {
	display: block;
	}
	#detail {
	display: none;
	}

	details summary > * {
		display: inline;
	}
	summary a * {
		pointer-events: none;
		} 
		details summary::-webkit-details-marker {
	display:none;
	}
	</style>
	
	<link rel="icon" href="misc/c_favicon1.png">

	<title>Chaitanya Chawla</title>
	<link href="https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic" rel="stylesheet" type="text/css">
	<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  	
	<!-- TANMAY's TAGS -->
	<!-- Google Tag Manager -->
	<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
	new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
	j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
	'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
	})(window,document,'script','dataLayer','GTM-NLLBQLT');</script>
	<!-- End Google Tag Manager -->

	<!-- Leaflet CSS -->
	<link rel="stylesheet" href="https://unpkg.com/leaflet/dist/leaflet.css" />
	<style>
	  #map {
	    height: 100vh; /* Adjust height as needed */
	  }
	</style>
	
</head>


<body>

	<!-- TANMAY's TAGS -->
	<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NLLBQLT"
	height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
	<!-- End Google Tag Manager (noscript) -->

  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
	<tbody><tr>
	  <td>

		<!-- ###################################################################### -->
		<!-- Intro -->
		<!-- ###################################################################### -->

		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
			<tbody><tr>
				<td width="95%" valign="middle">
			  	<p align="center">
					<name>Chaitanya Chawla</name>
					<!-- <name>Chaitanya Chawla <mid><font color=#FF0000>|</font></mid> <font color=#C0C0C0>Suddhu</font></name> -->
			  	<p align="center">
					<font size="3"> <b>Email:</b> cchawla <font color=#FF0000>[at]</font> cs <font color=#FF0000>[dot]</font> cmu <font color=#FF0000>[dot]</font> edu </font>
			  	</p>
			  	<p>
					<p style = "text-align:justify">Hi! I'm a Graduate Student Researcher at the <A href="http://www.ri.cmu.edu/" target="_blank">Robotics Institute</A> at <A href="http://www.cmu.edu/" target="_blank">Carnegie Mellon University</A>, where I am fortunate to be advised by <A href="http://www.gshi.me/" target="_blank">Prof. Guanya Shi</A>.
						I have also worked with <A href="https://www.cs.cmu.edu/~./jeanoh/" target="_blank">Prof. Jean Oh</A> as a visiting researcher. Previously, I graduated from the Technical University of Munich, as a 4-time reciepient of the <A href="https://www.deutschlandstipendium.de/deutschlandstipendium/de/services/english/the-deutschlandstipendium-best-of-both-worlds-for-students.html" target="_blank">German National Scholarship</A>.
			  	</p>
				
				<p>
					<p style = "text-align:justify">
						My research interests focus on learning-based robotic manipulation. Primarily, I am interested in exploring methods to incorporate human priors by learning skill representations across humans and robots.
			  	</p> 
				  <p style = "text-align:justify">
					When I'm not in the lab, I am either playing badminton üè∏ or <a href="map.html" target="_blank">travelling</a>. 
				</p> 


		  
			  	
			  <p align=center>
				<strong>
				<a href="misc/CV/Chaitanya_Chawla_Resume.pdf" target="_blank">CV</a> &nbsp/&nbsp
				<a href="https://scholar.google.com/citations?user=Q9zApYUAAAAJ&hl=en" target="_blank"> Google Scholar </a> &nbsp/&nbsp
				<a href="https://github.com/chaitanya1chawla/" target="_blank"> Github </a> &nbsp/&nbsp
				<a href="https://www.linkedin.com/in/chaitanya1chawla/" target="_blank"> LinkedIn </a> 
				</strong>
			  </p>
			</td>
			<td width="50%">
			  <img src="misc/chaitanya_portrait.png" style="width: 300; height: auto; border-radius: 5%;">
			</td>
		  </tr>
		</table>




		<!-- ###################################################################### -->
		<!-- Updates -->
		<!-- ###################################################################### -->

			<!-- <h2>Updates</h2>
		<table width="100%" align="center" border="0" cellspacing="6" cellpadding="0">
			<colgroup>
				<col span="1" style="width: 12%;">
				<col span="1" style="width: 88%;">
			</colgroup>
			<tbody>
			<tr>
				<td><p style="color:FF0000; display:inline;">[Sept '23] &nbsp</p></td>
				<td>Check out our real robot results on learning agent-environment interaction abstractions <a href="https://youtu.be/DhgBu8IYUEo"> here! </a></td>
			</tr>				
			<tr>
				<td><p style="color:FF0000; display:inline;">[Sept '23] &nbsp</p></td>
				<td>Submitted our work on learning agent-environment interaction abstractions to ICRA 2024!</td>
			</tr>
			<tr>
				<td><p style="color:FF0000; display:inline;">[June '23] &nbsp</p></td>
				<td>Presented my work on learning agent-environment interaction abstractions at the workshop on aligning human-robot representations at CoRL 2022.</td>
			</tr>
			<tr>
				<td><p style="color:FF0000; display:inline;">[Nov '22] &nbsp</p></td>
				<td>Successfully passed my Ph.D. thesis proposal! Here's a <a href="https://youtu.be/wbVPJU5p29w">recording of my talk!</a> </td> 
			</tr>
			   <tr>
				<td><p style="color:FF0000; display:inline">[Sep '22] &nbsp</p></td>
				<td><a href="https://suddhu.github.io/midastouch-tactile/">MidasTouch</a> was accepted to <a href="https://corl2022.org/">CoRL 2022</a> as an oral. 
				</td>
			  </tr>
		  </tbody>
		</tbody></table> -->
		</div>
		<br>
		<hr>
		
		<div style="height:20px;font-size:1px;">&nbsp;</div>

		<!-- ###################################################################### -->
		<!-- Research -->
		<!-- ###################################################################### -->

	  	
		
		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
		  <tr>
			<td width="100%" valign="middle">
			  <h2>Research</h2>
			</td>
		  </tr>
		</table>    

	  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">

		<!-- ###################################################################### -->
		<!-- HP~HP -->
		<!-- ###################################################################### -->

		<tr>
			<td width="25%" align="center">
				<!-- <img src='https://suddhu.github.io/midastouch-tactile/img/midastouch.gif' width="200"> -->
				<img src='data/media/GIFS/hat.avif' width="250">
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>
						Humanoid Policy ~ Human Policy
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					Ri-Zhao Qiu, Shiqi Yang, Xuxin Cheng, <b><u>Chaitanya Chawla</u></b>, <br>
					Jialong Li, Tairan He, Ge Yan, David J. Yoon, Ryan Hoque, Jian Zhang, Sha Yi, Guanya Shi, Xiaolong Wang
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<font color=#696969>arXiv 2025</font>
					<!-- <font color=#696969><em>International Conference on Machine Learning</em>, July 2022</font> -->
					<!-- <div style="height:5px;font-size:1px;">&nbsp;</div>
					[<font color=#009933>Oral: 6% acceptance rate</font>] -->
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<a href="https://arxiv.org/abs/2503.13441" target="_blank">Paper</a> /
					<a href="https://human-as-robot.github.io/" target="_blank">Website</a> /
					<a href="https://github.com/RogerQi/human-policy" target="_blank">Code</a>	/		
					<a href="https://huggingface.co/datasets/RogerQi/PH2D" target="_blank">Dataset</a>		
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id="detail">
				<i>
					<p><p style = "text-align:justify">
						Egocentric human demonstrations as a data source for humanoid manipulation.					</p>
				</div>
			</td>
		</tr> 


		<!-- ###################################################################### -->
		<!-- LIA -->
		<!-- ###################################################################### -->

		<tr>
			<td width="25%" align="center">
				<!-- <img src='https://suddhu.github.io/midastouch-tactile/img/midastouch.gif' width="200"> -->
				<img src='https://github.com/tanmayshankar/tanmayshankar.github.io/blob/master/data/media/GIFS/LIA3.gif?raw=true' width="250">
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>
						Translating Agent-Environment Interactions across Humans and Robots
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					Tanmay Shankar, <b><u>Chaitanya Chawla</u></b>, Almut Wakel, Jean Oh
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<font color=#696969>International Conference on Intelligent Robots and Systems,<br> IROS 2024</font>
					<!-- <font color=#696969><em>International Conference on Machine Learning</em>, July 2022</font> -->
					<!-- <div style="height:5px;font-size:1px;">&nbsp;</div>
					[<font color=#009933>Oral: 6% acceptance rate</font>] -->
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<a href="https://github.com/chaitanya1chawla/chaitanya1chawla.github.io/blob/master/data/papers/IROS_2024_Draft.pdf" target="_blank">Paper</a> /
					<!-- <a href="https://icml.cc/virtual/2022/spotlight/16262" target="_blank">Talk</a> / -->
					<a href="https://sites.google.com/view/interaction-abstractions" target="_blank">Website</a> /
					<a href="https://github.com/tanmayshankar/CausalSkillLearning" target="_blank">Code</a>	/		
					<a href="https://www.youtube.com/watch?v=amEA4JuZxzg" target="_blank">Video</a>
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id="detail">
				<i>
					<p><p style = "text-align:justify">
					We developed an unsupervised approach to learn temporal abstractions of skills incorporating agent-environment interactions. We hope to learn representations of patterns of motion of objects in the environment, or patterns of change of state. Our approach is able to learn semantically meaningful skill segments across robot and human demonstrations, despite being completely unsupervised.
					</p>
				</div>
			</td>
		</tr> 


		<!-- ###################################################################### -->
		<!-- RAF -->
		<!-- ###################################################################### -->
		
		<tr>
			<td width="25%" align="center">
				<!-- <img src='https://suddhu.github.io/midastouch-tactile/img/midastouch.gif' width="200"> -->
				<img src='https://github.com/tanmayshankar/tanmayshankar.github.io/blob/master/data/media/GIFS/TRS.gif?raw=true' width="300">
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>
						Robot-Agnostic Framework for One-Shot Intrinsic Feature Extraction
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<b><u>Chaitanya Chawla</u></b>,
					Andrei Costinescu,
					Darius Burschka
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<font color=#696969>In Preparation for IEEE Transactions on Knowledge and Data Engineering</font>
					<!-- <font color=#696969><em>International Conference on Machine Learning</em>, July 2022</font> -->
					<!-- <div style="height:5px;font-size:1px;">&nbsp;</div>
					[<font color=#009933>Oral: 6% acceptance rate</font>] -->
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<a href="data/papers/Engineering Practice Report_Teil2.pdf"_blank">Report</a> /
					<a href="https://docs.google.com/presentation/d/12YaqSte1F89_JjmZDsD0nDf6DWaXGfhfoEOrX2WlGs0/edit?usp=sharing" target="_blank">Presentation</a> /
					<a href="https://github.com/chaitanya1chawla/feat_extr_new" target="_blank">Code</a>					
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id="detail">
				<i>
					<p><p style = "text-align:justify">
					We developed an algorithmic framework to extract different intrinsic features from human demonstrations. We are studying various features, including interactions with objects along the trajectory, analyzing the environment for interactions with the background (e.g., wiping or writing), and classifying the type of motion within a trajectory segment (e.g., shaking, rotating, or transporting).
					</p>
				</div>
			</td>
		</tr> 


		<!-- ###################################################################### -->
		<!-- VTL -->
		<!-- ###################################################################### -->

		
		<tr>
			<td width="25%" align="center">
				<!-- <img src='https://suddhu.github.io/midastouch-tactile/img/midastouch.gif' width="200"> -->
				<img src='data/media/GIFS/combined_teleoperation.gif' width="300">
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>
					Visual Teleoperation using Dynamic Motion Primitives
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<b><u>Chaitanya Chawla</u></b>,
					Dongheui Lee
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<font color=#696969>Independent Research Project</font>
					<!-- <font color=#696969><em>International Conference on Machine Learning</em>, July 2022</font> -->
					<!-- <div style="height:5px;font-size:1px;">&nbsp;</div>
					[<font color=#009933>Oral: 6% acceptance rate</font>] -->
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<a href="data/papers/Visual_teleoperation_report.pdf" target="_blank">Report</a> /
					<a href="https://1drv.ms/p/c/bd9b89aec3b8be04/EQS-uMOuiZsggL2LAgAAAAABL5bv_Bw5HzWlxLbqeCI6QA?e=TYvaWK" target="_blank">Presentation</a> /
					<a href="https://github.com/chaitanya1chawla/Franka-Panda-Manipulation-via-Learning-from-demonstration" target="_blank">Code</a>					
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id="detail">
				<i>
					<p><p style = "text-align:justify">
					We presented a method to learn human motions using a Learning-From-Demonstration approach. 
					Using Dynamic Motion Primitives, we were able to teleoperate a Franka Panda Arm using the learned trajectories. 
					</p>

				</div>
			</td>
		</tr> 

	</table>

	<br>
	<div style="height:20px;font-size:1px;">&nbsp;</div>

	<hr>
	<br>


	<!-- ###################################################################### -->
	<!-- Projects -->
	<!-- ###################################################################### -->
		
	<h2>Projects</h2>

	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">

		<!-- ###################################################################### -->
		<!-- Face Recognition -->
		<!-- ###################################################################### -->

		<tr>
			<td width="25%" align="center">
				<img src="data/media/GIFS/all_chairs.gif" width="200">
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>				
						Self-supervised fine-tuning Pre-Grasps through 3D Object Generation
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<b><u>Chaitanya Chawla</u></b>,
					Almut Wakel, Eyob Dagnachew
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<font color=#696969>10-623: Carnegie Mellon University</font>
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<a href="data/papers/Final_report__10623.pdf"  target="_blank">Report</a> /
					<a href="https://github.com/almutwakel/3d-diffusion"  target="_blank">Code</a> /
					<a href="data/papers/10623 presentation.pdf"  target="_blank">Presentation</a>
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id="detail">
				<i>
					Comparing different methods including Autoencoders and PCA, for feature representation in face reconstruction 
				</div>
			</td>
		</tr> 


		<!-- ###################################################################### -->
		<!-- HOP -->
		<!-- ###################################################################### -->
			<tr>
				<td width="25%" align="center">
					<img src="data/media/GIFS/combined.gif" width="250">
				</td>
				<td valign="center" width="70%">
					<div id="summary">
						<papertitle>				
							Learning Dexterous Manipulation from Human Video Pretraining using 3D Point Tracks
						</papertitle>                   
						<div style="height:5px;font-size:1px;">&nbsp;</div>
						<b><u>Chaitanya Chawla</u></b>,
						Sungjae Park, Lucas Wu, Junkai Huang, Yanbo Xu
						<div style="height:5px;font-size:1px;">&nbsp;</div>
						<font color=#696969>16-831: Carnegie Mellon University</font>
						<!-- <font color=#696969><em>International Conference on Machine Learning</em>, July 2022</font> -->
						<!-- <div style="height:5px;font-size:1px;">&nbsp;</div>
						[<font color=#009933>Oral: 6% acceptance rate</font>] -->
						<div style="height:5px;font-size:1px;">&nbsp;</div>
						<a href="data/papers/IntroRobot_Project_ (1).pdf"  target="_blank">Report</a> /
						<a href="https://docs.google.com/presentation/d/1pnNjWGP6yWjrb_LHGNiWlOL8GEl1xVN43btW0dD-Z9Y/edit?slide=id.g3263b5c167b_0_0#slide=id.g3263b5c167b_0_0"  target="_blank">Presentation</a>
						<!-- <a href="https://github.com/tanmayshankar/RCNN_MDP" target="_blank">Code</a> 	-->
						<div style="height:15px;font-size:1px;">&nbsp;</div>
					</div>
					<div id="detail">
					<i>
						We proposed a pipeline to benchmark pre-training methods using different state representations.
						Our method consisted of extracting sensorimotor information from videos by lifting the human hand and the manipulated object in a
						shared 3D space in simulation (IsaacGym), i.e. either 3D point-tracks or 3D meshes.
						Then, we retarget hand-trajectories to a Franka with a Shadow hand.
						Finally, we fine-tune on various tasks.
					</div>
				</td>
			</tr> 

		<!-- ###################################################################### -->
		<!-- Face Recognition -->
		<!-- ###################################################################### -->

		<tr>
			<td width="25%" align="center">
				<img src="misc/face_reconstruction.png" width="300">
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>				
						Face Recognition using Autoencoders and PCA
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<b><u>Chaitanya Chawla</u></b>,
					Katherine Brenner
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<font color=#696969>7-835: Technical University of Munich</font>
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<a href="https://drive.google.com/file/d/1i4J1-dCMb8bOxTMC5gHboNliKuqBy0d_/view?usp=sharing"  target="_blank">Report</a>
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id="detail">
				<i>
					Comparing different methods including Autoencoders and PCA, for feature representation in face reconstruction 
				</div>
			</td>
		</tr> 

		<!-- ###################################################################### -->
		<!-- Halloween -->
		<!-- ###################################################################### -->

		<tr>
			<td width="25%" align="center">
				<img src='https://github.com/chaitanya1chawla/chaitanya1chawla.github.io/blob/master/data/media/GIFS/halloween_candy_robot.gif?raw=true' width="250", height="220">
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>				
					Candy Throwing Robot for Halloween 2023!
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<u>C. Chawla</u>
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<font color=#696969> 2 hours long Project on Halloween Eve, Bot Intelligence Group</font>
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id="detail">
				<i>
					Distributing candies during Halloween at the Robotics Institute, Carnegie Mellon University
				</div>
			</td>
		</tr> 

		<!-- ###################################################################### -->
		<!-- IWAMP -->
		<!-- ###################################################################### -->
		

	</table>

	<div style="height:20px;font-size:1px;">&nbsp;</div>

	<hr>
	<br>

	<h2>Work Experience</h2>

	<div class="experience">
		<!-- <h3>Robotics Engineer (Part-Time)</h3> -->
		<p><a href="https://www.reply.com/roboverse-reply/en" target="_blank">Roboverse Reply</a> | July 2023 - April 2024</p>
		
		<ul>
		  <li>Developed a perception pipeline for detecting and reporting measurements from analog gauges. Set up data-annotation, post-processing, and real-time inference of gauge measurements. Dockerized the application to integrate with Boston Dynamics' Spot to autonomously collect data in a factory environment.</li>
		  <br>
		  <li>Created a webRTC pipeline using gRPC to transfer point cloud data from Spot's LIDAR sensor to Oculus VR Headset, enabling a remote user to observe Spot's immediate environment in real time.</li>
		  <br>
		  <li>Migrated the company's robotic framework from ROS to ROS2.</li>
		</ul>
	  </div>

<!-- 
	<tr>
		<td width="25%" align="center">
			<img src='https://github.com/tanmayshankar/tanmayshankar.github.io/blob/master/data/media/GIFS/TRS.gif?raw=true' width="300">
		</td>
		<td valign="center" width="70%">
			<div id="summary">
				<papertitle>
					Robotics Engineer,  Part-Time
				</papertitle>                   
				<div style="height:5px;font-size:1px;">&nbsp;</div>
				<div style="height:5px;font-size:1px;">&nbsp;</div>
			</div>
			<div id="detail">
			<i>
				<p><p style = "text-align:justify">
				We developed an algorithmic framework to extract different intrinsic features from human demonstrations. We are studying various features, including interactions with objects along the trajectory, analyzing the environment for interactions with the background (e.g., wiping or writing), and classifying the type of motion within a trajectory segment (e.g., shaking, rotating, or transporting).
				</p>
			</div>
		</td>
	</tr>  -->

	<hr>
	<br>



	<h2>Achievements </h2>

	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
		  <tr>
			<td width="100%" valign="middle">
			</td>
		  </tr>

		  <u1>
			<li>German National Scholarship - Deutschland Stipendium,  2021, 22, 23, 24 </li>
			<li>Heinrich and Lotte Muhlfenzl Scholarship - undergraduate research scholarship, 2023 </li>
			<li>TUM PROMOS 2023 - merit scholarship for stay-abroad research, 2023 </li>
			<li>Max Weber Program - nominated by the university, 2022 </li>
			  </u1>

	</table>    

	<h2>Academic Service </h2>

	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
		  <tr>
			<td width="100%" valign="middle">
			</td>
		  </tr>

		  <u1>
			<li>Introduction to Robot Learning (16-831), Carnegie Mellon University, 2025 </li>
			<li>Robotic Control Laboratory (6-931), Technical University of Munich, 2023 </li>
			<li>Mathematical Analysis (9-411), Technical University of Munich, 2022 </li>
			  </u1>

	</table>    




<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td>
		<p align="right"><font size="2" color=#696969>
	Last updated: Jan 2023
		<p align="right"><font size="2" color=#696969>
<a href="http://www.cs.berkeley.edu/~barron/" target="_blank"><font size="2">Imitation is the highest form of flattery
</a>
</font></p>

<script type="text/javascript">
  var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
  document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
  try {
	var pageTracker = _gat._getTracker("UA-7580334-1");
	pageTracker._trackPageview();
  } catch (err) {}
</script>
</td></tr>
</table>

</body></html>

