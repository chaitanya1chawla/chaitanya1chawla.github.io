<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="en">

<head>
	<!-- <meta name=viewport content="width=800"> -->
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
	<meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
	<script type="text/javascript" src="js/hidebib.js"></script>
	<style type="text/css">
		/* Color scheme stolen from Sergey Karayev */
		
		hr {
		border: 0;
		height: 0;
		border-top: 1px solid rgba(0, 0, 0, 0.1);
		border-bottom: 1px solid rgba(255, 255, 255, 0.3);
		}

		img {
		border-radius: 5%;
		}

		a {
		color: #1772d0;
		text-decoration: none;
		}
		
		a:focus,
		a:hover {
		color: #f09228;
		text-decoration: none;
		}
		
		body,
		td,
		th,
		tr,
		p,
		/* a {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 14px
		}
		
		strong {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 14px;
		}
		
		heading {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 30px;
		}
		
		papertitle {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 14px;
		font-weight: 700;
		margin-bottom: 10cm;
		}
		
		name {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 32px;
		} */
		a {
		font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
		font-size: 16px;
		font-weight: 400
		}

		strong {
		font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
		font-size: 16px;
		font-weight: 600
		}

		heading {
		font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
		font-size: 17px;
		font-weight: 600
		}

		papertitle {
		font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
		font-size: 16px;
		font-weight: 600
		}

		name {
		font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
		font-size: 32px;
		font-weight: 400
		}
		.zero {
		width: 160px;
		height: 80px;
		position: relative;
		}

		.one {
		width: 160px;
		height: 160px;
		position: relative;
		}
		
		.two {
		width: 160px;
		height: 160px;
		position: absolute;
		transition: opacity .2s ease-in-out;
		-moz-transition: opacity .2s ease-in-out;
		-webkit-transition: opacity .2s ease-in-out;
		}
		
		.fade {
		transition: opacity .2s ease-in-out;
		-moz-transition: opacity .2s ease-in-out;
		-webkit-transition: opacity .2s ease-in-out;
		}
		
		mid {
		font-size: 40px;
		position:relative;
		top:2px;
		}

		span.highlight {
		background-color: #ffffd0;
		}


	#summary:hover + #detail, #detail:hover {
	display: block;
	}
	#detail {
	display: none;
	}

	details summary > * {
		display: inline;
	}
	summary a * {
		pointer-events: none;
		} 
		details summary::-webkit-details-marker {
	display:none;
	}
	</style>
	
	<link rel="icon" href="misc/seal-4c-600x600-min-photoaidcom-cropped.jpg">

	<title>Chaitanya Chawla</title>
	<link href="https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic" rel="stylesheet" type="text/css">
	<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  	
	<!-- TANMAY's TAGS -->
	<!-- Google Tag Manager -->
	<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
	new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
	j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
	'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
	})(window,document,'script','dataLayer','GTM-NLLBQLT');</script>
	<!-- End Google Tag Manager -->

	<!-- Leaflet CSS -->
	<link rel="stylesheet" href="https://unpkg.com/leaflet/dist/leaflet.css" />
	<style>
	  #map {
	    height: 100vh; /* Adjust height as needed */
	  }
	</style>
	
</head>


<body>

	<!-- TANMAY's TAGS -->
	<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NLLBQLT"
	height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
	<!-- End Google Tag Manager (noscript) -->

  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
	<tbody><tr>
	  <td>

		<!-- ###################################################################### -->
		<!-- Intro -->
		<!-- ###################################################################### -->

		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="0" style="margin-top: 40px;">
			<tbody><tr>
				<td width="95%" valign="middle" style="padding-right: 0px; font-size: 14px;">
			  	<p align="center">
					<name>Chaitanya Chawla</name>
					<!-- <name>Chaitanya Chawla <mid><font color=#FF0000>|</font></mid> <font color=#C0C0C0>Suddhu</font></name> -->
			  	<p align="center">
					<font size="2.5"> <b>Email:</b> cchawla <font color=#FF0000>[at]</font> cs <font color=#FF0000>[dot]</font> cmu <font color=#FF0000>[dot]</font> edu </font>
			  	</p>
			  	<p>
					<font size="3">
					<p style = "text-align:justify">Hi! I'm a Graduate Student Researcher at the \
						<A href="http://www.ri.cmu.edu/" target="_blank">Robotics Institute</A> at \
						<A href="http://www.cmu.edu/" target="_blank">Carnegie Mellon University</A>, \
						advised by <A href="http://www.gshi.me/" target="_blank">Prof. Guanya Shi</A>.
						I have also worked with <A href="https://www.cs.cmu.edu/~./jeanoh/" target="_blank">Prof. Jean Oh</A> as a visiting researcher. \\
						<br>
						Previously, I completed my B.S. from T.U. Munich, where I was a 4-time recipient of the \
						<A href="https://www.deutschlandstipendium.de/deutschlandstipendium/de/services/english/the-deutschlandstipendium-best-of-both-worlds-for-students.html" target="_blank">German National Scholarship</A>.
					</font>
			  	</p>
				
				<p>
					<font size="3">
					<p style = "text-align:justify">
						My research interests focus on learning-based robotic manipulation. \
						Primarily, I am interested in exploring methods to incorporate human priors by learning skill representations across humans and robots.
					</font>
			  	</p> 
					<font size="3">
				  <p style = "text-align:justify">
					When I'm not in the lab, I am either playing badminton üè∏ or <a href="map.html" target="_blank">travelling</a>. 
				</font>
			</p> 


		  
			  	
			  <p align=center>
				<strong>
				<a href="misc/CV/Chaitanya_Chawla_CV_CMU_(1).pdf" target="_blank">CV</a> &nbsp/&nbsp
				<a href="https://scholar.google.com/citations?user=Q9zApYUAAAAJ&hl=en" target="_blank"> Google Scholar </a> &nbsp/&nbsp
				<a href="https://github.com/chaitanya1chawla/" target="_blank"> Github </a> &nbsp/&nbsp
				<a href="https://www.linkedin.com/in/chaitanya1chawla/" target="_blank"> LinkedIn </a> 
				</strong>
			  </p>
			</td>
			
			<td width="50%" style="padding-left: 25px;">
			  <img src="misc/chaitanya_portrait-photoaidcom-cropped.png" style="width: 300; height: auto; border-radius: 5%;">
			</td>
		  </tr>
		</table>




		<!-- ###################################################################### -->
		<!-- Updates -->
		<!-- ###################################################################### -->

			<!-- <h2>Updates</h2>
		<table width="100%" align="center" border="0" cellspacing="6" cellpadding="0">
			<colgroup>
				<col span="1" style="width: 12%;">
				<col span="1" style="width: 88%;">
			</colgroup>
			<tbody>
			<tr>
				<td><p style="color:FF0000; display:inline;">[Sept '23] &nbsp</p></td>
				<td>Check out our real robot results on learning agent-environment interaction abstractions <a href="https://youtu.be/DhgBu8IYUEo"> here! </a></td>
			</tr>				
			<tr>
				<td><p style="color:FF0000; display:inline;">[Sept '23] &nbsp</p></td>
				<td>Submitted our work on learning agent-environment interaction abstractions to ICRA 2024!</td>
			</tr>
			<tr>
				<td><p style="color:FF0000; display:inline;">[June '23] &nbsp</p></td>
				<td>Presented my work on learning agent-environment interaction abstractions at the workshop on aligning human-robot representations at CoRL 2022.</td>
			</tr>
			<tr>
				<td><p style="color:FF0000; display:inline;">[Nov '22] &nbsp</p></td>
				<td>Successfully passed my Ph.D. thesis proposal! Here's a <a href="https://youtu.be/wbVPJU5p29w">recording of my talk!</a> </td> 
			</tr>
			   <tr>
				<td><p style="color:FF0000; display:inline">[Sep '22] &nbsp</p></td>
				<td><a href="https://suddhu.github.io/midastouch-tactile/">MidasTouch</a> was accepted to <a href="https://corl2022.org/">CoRL 2022</a> as an oral. 
				</td>
			  </tr>
		  </tbody>
		</tbody></table> -->
		</div>
		<br>
		<hr>
		
		<div style="height:20px;font-size:1px;">&nbsp;</div>

		<!-- ###################################################################### -->
		<!-- Research -->
		<!-- ###################################################################### -->

	  	
		
		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
		  <tr>
			<td width="100%" valign="middle">
			  <h2>Research</h2>
			</td>
		  </tr>
		</table>    

	  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">


		<!-- ###################################################################### -->
		<!-- Mayavi -->
		<!-- ###################################################################### -->

		<tr>
			<td width="25%" align="center">
				<img src='data/media/GIFS/mayavi.gif' width="250">
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>
						Robots Learning from Humans, Humans Learning from Robots
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<b>Chaitanya Chawla</b>, Ananya Bal, Laszlo Jeni, Guanya Shi
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<font color=#696969>In Progress</font>
					<!-- <font color=#696969><em>International Conference on Machine Learning</em>, July 2022</font> -->
					<!-- <div style="height:5px;font-size:1px;">&nbsp;</div>
					[<font color=#009933>Oral: 6% acceptance rate</font>] -->
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<!-- <a href="data/papers/skill_graph_documentation.pdf" target="_blank">Technical Report</a> / -->
					<!-- <a href="https://human-as-robot.github.io/" target="_blank">Website</a> / -->
					<a href="https://github.com/chaitanya1chawla/dexmachina-humanoid" target="_blank">Code (coming soon)</a>		
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id="detail">
				<i>
					<p><p style = "text-align:justify">
						Robots learning from humans, humans learning from robots. Stay tuned!
						</p>
				</div>
			</td>
		</tr> 


		<!-- ###################################################################### -->
		<!-- Data Engine -->
		<!-- ###################################################################### -->

		<tr>
			<td width="25%" align="center">
				<img src='data/media/Images/Gemini_Generated_Image_bvpjeobvpjeobvpj-removebg-preview.png' width="250">
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>
						Data Engine for Web-Scale Robot Training
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<b>Chaitanya Chawla*</b>, Jeremy Collins*, Seungjae Lee*, <br>
					Rutav Shah, Krishnan Srinivasan, Homanga Bharadwaj
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<font color=#696969>In Progress</font>
					<!-- <font color=#696969><em>International Conference on Machine Learning</em>, July 2022</font> -->
					<!-- <div style="height:5px;font-size:1px;">&nbsp;</div>
					[<font color=#009933>Oral: 6% acceptance rate</font>] -->
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<!-- <a href="data/papers/skill_graph_documentation.pdf" target="_blank">Technical Report</a> / -->
					<!-- <a href="https://human-as-robot.github.io/" target="_blank">Website</a> / -->
					<a href="https://github.com/jeremy-collins/yt-mr" target="_blank">Code (coming soon)</a>	
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id="detail">
				<i>
					<p><p style = "text-align:justify">
						Building a data engine for web-scale robot training. Stay tuned!
					</p>
				</div>
			</td>
		</tr> 


		<!-- ###################################################################### -->
		<!-- SG -->
		<!-- ###################################################################### -->

		<tr>
			<td width="25%" align="center">
				<!-- <img src='https://suddhu.github.io/midastouch-tactile/img/midastouch.gif' width="200"> -->
				<img src='data/media/GIFS/skillgraph.gif' width="250">
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>
						SkillGraph: An Ontology-Based Framework for Intelligent Multi-Robot Assembly and Planning
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					Peiqi Liu*, Philip Huang*, <b>Chaitanya Chawla*</b>, <br>
					Guanya Shi, Jiaoyang Li, Changliu Liu
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<font color=#696969>In Preparation for Robotics and Automation Letters (RA-L)</font>
					<!-- <font color=#696969><em>International Conference on Machine Learning</em>, July 2022</font> -->
					<!-- <div style="height:5px;font-size:1px;">&nbsp;</div>
					[<font color=#009933>Oral: 6% acceptance rate</font>] -->
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<a href="data/papers/skill_graph_documentation.pdf" target="_blank">Technical Report</a> /
					<!-- <a href="https://human-as-robot.github.io/" target="_blank">Website</a> / -->
					<a href="https://github.com/intelligent-control-lab/AIDF" target="_blank">Code (coming soon)</a>	
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id="detail">
				<i>
					<p><p style = "text-align:justify">
						Developed SkillGraph, an ontology-driven framework that unifies semantic knowledge representation 
						with automated task planning to enable flexible multi-robot assembly. 
						By bridging high-level reasoning with low-level control, the system dynamically maps abstract task 
						goals into executable skill primitives, ensuring robust manipulation in complex environments. 
						The SkillGraph comprises of multiple user-interfaces to interact with the system, one of which is a human-video demonstration 
						‚Äîwhere the system is able to learn the skill from the human-video demonstration and then use it to plan and execute the task.
						</p>
				</div>
			</td>
		</tr> 


		<!-- ###################################################################### -->
		<!-- HP~HP -->
		<!-- ###################################################################### -->

		<tr>
			<td width="25%" align="center">
				<!-- <img src='https://suddhu.github.io/midastouch-tactile/img/midastouch.gif' width="200"> -->
				<img src='data/media/GIFS/hat.avif' width="250">
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>
						Humanoid Policy ~ Human Policy
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					Ri-Zhao Qiu*, Shiqi Yang*, Xuxin Cheng*, <b>Chaitanya Chawla*</b>, <br>
					Jialong Li, Tairan He, Ge Yan, David J. Yoon, Ryan Hoque, Jian Zhang, Sha Yi, Guanya Shi, Xiaolong Wang
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<font color=#696969>Conference on Robot Learning, CoRL 2025</font>
					<font color=#696969>Robot Data Workshop @ CoRL 2025</font>
					<!-- <font color=#696969><em>International Conference on Machine Learning</em>, July 2022</font> -->
					<!-- <div style="height:5px;font-size:1px;">&nbsp;</div>
					[<font color=#009933>Oral: 6% acceptance rate</font>] -->
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<a href="https://arxiv.org/abs/2503.13441" target="_blank">Paper</a> /
					<a href="https://human-as-robot.github.io/" target="_blank">Website</a> /
					<a href="https://github.com/RogerQi/human-policy" target="_blank">Code</a>	/		
					<a href="https://huggingface.co/datasets/RogerQi/PH2D" target="_blank">Dataset</a>		
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id="detail">
				<i>
					<p><p style = "text-align:justify">
						Developed HAT, an end-to-end policy that models human demonstrators as a distinct humanoid embodiment 
						to enable scalable imitation learning. The framework utilizes a unified kinematic representation to 
						directly map egocentric human video to robot actions, bypassing the need for complex intermediate 
						affordance representations. This approach allows for efficient learning from diverse human demonstrations 
						‚Äîsignificantly improving policy robustness and out-of-distribution generalization for bimanual manipulation.</p>
				</div>
			</td>
		</tr> 


		<!-- ###################################################################### -->
		<!-- LIA -->
		<!-- ###################################################################### -->

		<tr>
			<td width="25%" align="center">
				<!-- <img src='https://suddhu.github.io/midastouch-tactile/img/midastouch.gif' width="200"> -->
				<img src='https://github.com/tanmayshankar/tanmayshankar.github.io/blob/master/data/media/GIFS/LIA3.gif?raw=true' width="250">
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>
						Translating Agent-Environment Interactions across Humans and Robots
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					Tanmay Shankar, <b>Chaitanya Chawla</b>, Almut Wakel, Jean Oh
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<font color=#696969>International Conference on Intelligent Robots and Systems,<br> IROS 2024</font>
					<!-- <font color=#696969><em>International Conference on Machine Learning</em>, July 2022</font> -->
					<!-- <div style="height:5px;font-size:1px;">&nbsp;</div>
					[<font color=#009933>Oral: 6% acceptance rate</font>] -->
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10802752" target="_blank">Paper</a> /
					<!-- <a href="https://icml.cc/virtual/2022/spotlight/16262" target="_blank">Talk</a> / -->
					<a href="https://sites.google.com/view/interaction-abstractions" target="_blank">Website</a> /
					<a href="https://github.com/tanmayshankar/CausalSkillLearning" target="_blank">Code</a>	/		
					<a href="https://www.youtube.com/watch?v=amEA4JuZxzg" target="_blank">Video</a>
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id="detail">
				<i>
					<p><p style = "text-align:justify">
					We developed an unsupervised approach to learn temporal abstractions of skills incorporating 
					agent-environment interactions. We hope to learn representations of patterns of motion of objects 
					in the environment, or patterns of change of state. Our approach is able to learn semantically meaningful 
					skill segments across robot and human demonstrations, despite being completely unsupervised.
					</p>
				</div>
			</td>
		</tr> 


		<!-- ###################################################################### -->
		<!-- RAF -->
		<!-- ###################################################################### -->
		
		<tr>
			<td width="25%" align="center">
				<!-- <img src='https://suddhu.github.io/midastouch-tactile/img/midastouch.gif' width="200"> -->
				<img src='https://github.com/tanmayshankar/tanmayshankar.github.io/blob/master/data/media/GIFS/TRS.gif?raw=true' width="300">
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>
						Robot-Agnostic Framework for One-Shot Intrinsic Feature Extraction
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<b>Chaitanya Chawla</b>,
					Andrei Costinescu,
					Darius Burschka
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<font color=#696969>Arxiv 2023</font>
					<!-- <font color=#696969><em>International Conference on Machine Learning</em>, July 2022</font> -->
					<!-- <div style="height:5px;font-size:1px;">&nbsp;</div>
					[<font color=#009933>Oral: 6% acceptance rate</font>] -->
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<a href="data/papers/Engineering Practice Report_Teil2.pdf"_blank">Report</a> /
					<a href="https://docs.google.com/presentation/d/12YaqSte1F89_JjmZDsD0nDf6DWaXGfhfoEOrX2WlGs0/edit?usp=sharing" target="_blank">Presentation</a> /
					<a href="https://github.com/chaitanya1chawla/feat_extr_new" target="_blank">Code</a>					
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id="detail">
				<i>
					<p><p style = "text-align:justify">
					We developed an algorithmic framework to extract different intrinsic features from human 
					demonstrations. We are studying various features, including interactions with objects 
					along the trajectory, analyzing the environment for interactions with the background 
					(e.g., wiping or writing), and classifying the type of motion within a trajectory 
					segment (e.g., shaking, rotating, or transporting).
					</p>
				</div>
			</td>
		</tr> 


		<!-- ###################################################################### -->
		<!-- VTL -->
		<!-- ###################################################################### -->

		
		<tr>
			<td width="25%" align="center">
				<!-- <img src='https://suddhu.github.io/midastouch-tactile/img/midastouch.gif' width="200"> -->
				<img src='data/media/GIFS/combined_teleoperation.gif' width="300">
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>
					Visual Teleoperation using Dynamic Motion Primitives
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<b>Chaitanya Chawla</b>,
					Dongheui Lee
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<font color=#696969>Independent Research Project</font>
					<!-- <font color=#696969><em>International Conference on Machine Learning</em>, July 2022</font> -->
					<!-- <div style="height:5px;font-size:1px;">&nbsp;</div>
					[<font color=#009933>Oral: 6% acceptance rate</font>] -->
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<a href="data/papers/Visual_teleoperation_report.pdf" target="_blank">Report</a> /
					<a href="https://1drv.ms/p/c/bd9b89aec3b8be04/EQS-uMOuiZsggL2LAgAAAAABL5bv_Bw5HzWlxLbqeCI6QA?e=TYvaWK" target="_blank">Presentation</a> /
					<a href="https://github.com/chaitanya1chawla/Franka-Panda-Manipulation-via-Learning-from-demonstration" target="_blank">Code</a>					
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id="detail">
				<i>
					<p><p style = "text-align:justify">
					We presented a method to learn human motions using a Learning-From-Demonstration approach. 
					Using Dynamic Motion Primitives, we were able to teleoperate a Franka Panda Arm using the learned trajectories. 
					</p>

				</div>
			</td>
		</tr> 

	</table>

	<br>
	<div style="height:20px;font-size:1px;">&nbsp;</div>

	<hr>
	<br>


	<!-- ###################################################################### -->
	<!-- Projects -->
	<!-- ###################################################################### -->
		
	<h2>Projects</h2>

	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">

		<!-- ###################################################################### -->
		<!-- VLA Finetuning -->
		<!-- ###################################################################### -->

		<tr>
			<td width="25%" align="center">
				<img src="data/media/Images/Gemini_Generated_Image_bvpjeobvpjeobvpj-removebg-preview.png" width="200">
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>				
						Multi-GPU SpeedUp with Custom-implementation in Needle Framework (DDP and ZeRO-3)
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					Yifu Yuan, Yuanhang Zhang, <b>Chaitanya Chawla</b>
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<font color=#696969>10-714: Deep Learning Systems</font>
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<a href="data/ddp.ipynb"  target="_blank">Report</a> /
					<a href="https://github.com/hang0610/10414-DLS-Project"  target="_blank">Code (Ask me for access)</a> /
					<a href="data/media/vids/10714-needle-ddp.mp4"  target="_blank">Video</a>
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id="detail">
				<i>
					Multi-GPU SpeedUp with Custom-implementation in Needle Framework (DDP and ZeRO-3)
				</div>
			</td>
		</tr> 


		<!-- ###################################################################### -->
		<!-- Pre-grasp finetuning -->
		<!-- ###################################################################### -->

		<tr>
			<td width="25%" align="center">
				<img src="data/media/GIFS/all_chairs.gif" width="200">
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>				
						Self-supervised fine-tuning Pre-Grasps through 3D Object Generation
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<b>Chaitanya Chawla</b>,
					Almut Wakel, Eyob Dagnachew
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<font color=#696969>10-623: Generative AI</font>
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<a href="data/papers/Final_report__10623.pdf"  target="_blank">Report</a> /
					<a href="https://github.com/almutwakel/3d-diffusion"  target="_blank">Code</a> /
					<a href="data/papers/10623 presentation.pdf"  target="_blank">Presentation</a>
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id="detail">
				<i>
					Comparing different methods including Autoencoders and PCA, for feature representation in face reconstruction 
				</div>
			</td>
		</tr> 


		<!-- ###################################################################### -->
		<!-- HOP -->
		<!-- ###################################################################### -->
			<tr>
				<td width="25%" align="center">
					<img src="data/media/GIFS/combined.gif" width="250">
				</td>
				<td valign="center" width="70%">
					<div id="summary">
						<papertitle>				
							Learning Dexterous Manipulation from Human Video Pretraining using 3D Point Tracks
						</papertitle>                   
						<div style="height:5px;font-size:1px;">&nbsp;</div>
						<b>Chaitanya Chawla</b>,
						Sungjae Park, Lucas Wu, Junkai Huang, Yanbo Xu
						<div style="height:5px;font-size:1px;">&nbsp;</div>
						<font color=#696969>16-831: Introduction to Robot Learning</font>
						<!-- <font color=#696969><em>International Conference on Machine Learning</em>, July 2022</font> -->
						<!-- <div style="height:5px;font-size:1px;">&nbsp;</div>
						[<font color=#009933>Oral: 6% acceptance rate</font>] -->
						<div style="height:5px;font-size:1px;">&nbsp;</div>
						<a href="data/papers/IntroRobot_Project_ (1).pdf"  target="_blank">Report</a> /
						<a href="https://docs.google.com/presentation/d/1pnNjWGP6yWjrb_LHGNiWlOL8GEl1xVN43btW0dD-Z9Y/edit?slide=id.g3263b5c167b_0_0#slide=id.g3263b5c167b_0_0"  target="_blank">Presentation</a>
						<!-- <a href="https://github.com/tanmayshankar/RCNN_MDP" target="_blank">Code</a> 	-->
						<div style="height:15px;font-size:1px;">&nbsp;</div>
					</div>
					<div id="detail">
					<i>
						We proposed a pipeline to benchmark pre-training methods using different state representations.
						Our method consisted of extracting sensorimotor information from videos by lifting the human hand and the manipulated object in a
						shared 3D space in simulation (IsaacGym), i.e. either 3D point-tracks or 3D meshes.
						Then, we retarget hand-trajectories to a Franka with a Shadow hand.
						Finally, we fine-tune on various tasks.
					</div>
				</td>
			</tr> 

		<!-- ###################################################################### -->
		<!-- Face Recognition -->
		<!-- ###################################################################### -->

		<!-- <tr>
			<td width="25%" align="center">
				<img src="misc/face_reconstruction.png" width="300">
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>				
						Face Recognition using Autoencoders and PCA
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>
						<b>Chaitanya Chawla</b>,
					Katherine Brenner
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<font color=#696969>7-835: Technical University of Munich</font>
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<a href="https://drive.google.com/file/d/1i4J1-dCMb8bOxTMC5gHboNliKuqBy0d_/view?usp=sharing"  target="_blank">Report</a>
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id="detail">
				<i>
					Comparing different methods including Autoencoders and PCA, for feature representation in face reconstruction 
				</div>
			</td>
		</tr>  -->

		<!-- ###################################################################### -->
		<!-- Halloween -->
		<!-- ###################################################################### -->

		<tr>
			<td width="25%" align="center">
				<img src='https://github.com/chaitanya1chawla/chaitanya1chawla.github.io/blob/master/data/media/GIFS/halloween_candy_robot.gif?raw=true' width="250", height="220">
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>				
					Candy Throwing Robot for Halloween 2023!
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<b>C. Chawla</b>
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<font color=#696969> 2 hours long Project on Halloween Eve, Bot Intelligence Group</font>
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id="detail">
				<i>
					Distributing candies during Halloween at the Robotics Institute, Carnegie Mellon University
				</div>
			</td>
		</tr> 

		<!-- ###################################################################### -->
		<!-- IWAMP -->
		<!-- ###################################################################### -->
		

	</table>

	<div style="height:20px;font-size:1px;">&nbsp;</div>

	<hr>
	<br>

	<h2>Work Experience</h2>

	<div class="experience">
		<!-- <h3>Robotics Engineer (Part-Time)</h3> -->
		<p><a href="https://www.reply.com/roboverse-reply/en" target="_blank">Roboverse Reply</a> | July 2023 - April 2024</p>
		
		<ul>
		  <li>Developed a perception pipeline for detecting and reporting measurements from analog gauges. 
			Set up data-annotation, post-processing, and real-time inference of gauge measurements. 
			Dockerized the application to integrate with Boston Dynamics' Spot to autonomously collect data 
			in a factory environment.</li>
		  <br>
		  <li>Created a webRTC pipeline using gRPC to transfer point cloud data from Spot's 
			LIDAR sensor to Oculus VR Headset, enabling a remote user to observe Spot's immediate 
			environment in real time.</li>
		  <br>
		  <li>Migrated the company's robotic framework from ROS to ROS2.</li>
		</ul>
	  </div>

<!-- 
	<tr>
		<td width="25%" align="center">
			<img src='https://github.com/tanmayshankar/tanmayshankar.github.io/blob/master/data/media/GIFS/TRS.gif?raw=true' width="300">
		</td>
		<td valign="center" width="70%">
			<div id="summary">
				<papertitle>
					Robotics Engineer,  Part-Time
				</papertitle>                   
				<div style="height:5px;font-size:1px;">&nbsp;</div>
				<div style="height:5px;font-size:1px;">&nbsp;</div>
			</div>
			<div id="detail">
			<i>
				<p><p style = "text-align:justify">
				We developed an algorithmic framework to extract different intrinsic 
				features from human demonstrations. We are studying various features, 
				including interactions with objects along the trajectory, analyzing the 
				environment for interactions with the background (e.g., wiping or writing),
				and classifying the type of motion within a trajectory segment 
				(e.g., shaking, rotating, or transporting).
				</p>
			</div>
		</td>
	</tr>  -->

	<hr>
	<br>



	<h2>Achievements </h2>

	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
		  <tr>
			<td width="100%" valign="middle">
			</td>
		  </tr>

		  <u1>
			<li>German National Scholarship - Deutschland Stipendium,  2021, 22, 23, 24 </li>
			<li>Heinrich and Lotte Muhlfenzl Scholarship - undergraduate research scholarship, 2023 </li>
			<li>TUM PROMOS 2023 - merit scholarship for stay-abroad research, 2023 </li>
			<li>Max Weber Program - nominated by the university, 2022 </li>
			  </u1>

	</table>    

	<h2>Reviewer </h2>

	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
		  <tr>
			<td width="100%" valign="middle">
			</td>
		  </tr>

		  <u1>
			<li>Robotics and Automation Letters (RA-L) </li>
			<li>International Conference on Learning Representations (ICLR) </li>
			<li>Conference on Robot Learning (CoRL) </li>
			<li>International Conference on Intelligent Robots and Systems (IROS) </li>
			  </u1>

	</table>    

	<h2>Teaching Assistant </h2>

	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
		  <tr>
			<td width="100%" valign="middle">
			</td>
		  </tr>

		  <u1>
			<li>Introduction to Robot Learning (16-831), Carnegie Mellon University, 2025 </li>
			<li>Robotic Control Laboratory (6-931), Technical University of Munich, 2023 </li>
			<li>Mathematical Analysis (9-411), Technical University of Munich, 2022 </li>
			  </u1>

	</table>    




<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td>
		<p align="right"><font size="2" color=#696969>
	Last updated: Jan 2023
		<p align="right"><font size="2" color=#696969>
<a href="http://www.cs.berkeley.edu/~barron/" target="_blank"><font size="2">Imitation is the highest form of flattery
</a>
</font></p>

<script type="text/javascript">
  var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
  document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
  try {
	var pageTracker = _gat._getTracker("UA-7580334-1");
	pageTracker._trackPageview();
  } catch (err) {}
</script>
</td></tr>
</table>

</body></html>

